<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AI-Counselling: Virtual Mental Health Companion</title>
  <link rel="stylesheet" href="../css/normalize.css" />
  <link rel="stylesheet" href="../css/style.css" />
  <link rel="stylesheet" href="../css/font-root-styles/general-sans.css" />
  <link rel="stylesheet" href="../css/font-root-styles/okinesans.css" />
  <link rel="stylesheet" href="../css/font-root-styles/satoshi.css" />
  <link rel="stylesheet" href="../css/ai_councelling.css" />
</head>
<body>

<header>
  <h1>AI-Counselling</h1>
  <div class="separator"></div>
</header>

<main>
  <article>
    <section class="content-section">
      <h2>Overview</h2>
      <p>
        AI-Counselling is a virtual mental-health companion that supports users across text chat, voice, and video.
        The system combines real-time emotion detection, speech-to-text, tone analysis, and retrieval-augmented
        generation (RAG) grounded in the DSM-5 to deliver supportive, empathetic responses. The frontend is built in
        Next.js with Clerk authentication and a personalized onboarding flow, while a FastAPI backend handles media
        processing and AI inference.
      </p>
      <p>
        From a data-science perspective, the product is a <strong>multimodal pipeline</strong>: audio and video streams are collected,
        cleaned, normalized, and converted into structured signals (transcripts, emotion labels, confidence scores),
        then fused with a retrieval layer that anchors the LLM to clinically relevant context.
      </p>
    </section>

    <section class="content-section">
      <h2>Problem and Goal</h2>
      <p>
        People often need immediate, low-friction emotional support before they are ready to seek professional help.
        This project focuses on creating a safe, accessible companion that listens, reflects, and offers practical
        guidance while remaining clear about its boundaries. It is designed to complement care and encourage users
        to seek licensed help when appropriate.
      </p>
    </section>

    <section class="content-section">
      <h2>Core User Experience</h2>
      <p>
        Users can choose a preferred mode (text, voice, or video) during onboarding. The experience adapts to their
        choice, while still using a consistent safety policy and a shared knowledge base.
      </p>
      <ul>
        <li><strong>Text:</strong> A chat interface that sends user messages to the backend and returns an empathetic response.</li>
        <li><strong>Voice:</strong> A voice recorder streams audio chunks to cloud storage and triggers speech processing.</li>
        <li><strong>Video:</strong> A video recorder captures facial expressions and integrates tone analysis and transcript context.</li>
      </ul>
      <p>
        The onboarding wizard collects high-level context—such as age range, concerns, goals, and cross-cutting symptoms.
        This information is stored in MongoDB and used to tailor responses, ensuring the assistant adapts to the user's specific needs
        rather than offering generic advice.
      </p>
    </section>

    <section class="content-section">
      <h2>Architecture</h2>
      <p>
        The system uses a Next.js frontend connected to a FastAPI backend. Media is uploaded to Google Cloud Storage,
        and MongoDB stores user profiles and chat history. The backend performs emotion and tone analysis, speech
        transcription, and retrieval-augmented generation with the DSM-5.
      </p>
      <figure>
        <img src="../img/ai_counceling/architecture_light.png" alt="AI-Counselling architecture diagram" />
        <figcaption>Figure 1: High-level architecture fusing multimodal inputs with RAG.</figcaption>
      </figure>
    </section>

    <section class="content-section">
      <h2>Retrieval-Augmented Generation (RAG)</h2>
      <p>
        To prevent hallucinations, the system relies on the DSM-5 as a ground-truth knowledge base. The text is chunked into
        short passages and embedded using <strong>Sentence Transformers</strong>.
      </p>
      <p>
        <strong>Technical Strategy:</strong> The chunking strategy uses fixed-size word windows (300 words per chunk).
        Embeddings are normalized and queried with cosine similarity via <strong>FAISS IndexFlatIP</strong>, returning the top-K (K=5) passages.
        This lightweight vector database avoids external services while enabling fast, in-memory semantic retrieval for each request.
      </p>
    </section>

    <section class="content-section">
      <h2>Voice Analysis Pipeline</h2>
      <p>
        The voice flow records short audio chunks in the browser and uploads them to GCS. The backend downloads recent
        chunks, normalizes the audio, and performs speech emotion recognition using <strong>Wav2Vec2</strong>.
      </p>
      <p>
        <strong>Signal Processing:</strong> On ingestion, audio is resampled to 16 kHz mono and passed through high/low-pass
        filters (100 Hz–8 kHz) to remove noise. The audio is then normalized and compressed for dynamic range stability.
      </p>
      <p>
        <strong>Ensemble Classification:</strong> The recognizer uses an ensemble strategy where each 1-second chunk is
        evaluated multiple times with a Wav2Vec2 emotion classifier. These results are aggregated using majority voting
        and confidence weighting to determine the dominant emotional state (e.g., "Anxious", "Neutral").
      </p>
    </section>

    <section class="content-section">
      <h2>Speech-to-Text Robustness</h2>
      <p>
        The transcription path supports noisy, real-world audio by attempting tolerant decoding (FFmpeg) when direct
        parsing fails. Audio is chunked into 50-second segments to reduce API errors and improve recognition accuracy,
        then stitched into a single transcript for retrieval and response generation.
      </p>
    </section>

    <section class="content-section">
      <h2>Video Emotion Detection</h2>
      <p>
        Video recordings are processed server-side with OpenCV. Each frame is analyzed with <strong>DeepFace</strong> to detect
        dominant facial emotions.
      </p>
      <p>
        <strong>Latency Optimization:</strong> DeepFace runs per-frame inference with detection fallback disabled to avoid
        hard failures on low-quality frames or motion blur. The system aggregates the per-frame labels into a dominant emotion
        using a simple frequency model, balancing robustness and latency for real-time feedback.
      </p>
    </section>

    <section class="content-section">
      <h2>LLM Prompting & Fusion</h2>
      <p>
        The final response is composed from multiple signals: user text/transcript, detected facial emotion,
        Wav2Vec2 tone analysis, relevant DSM-5 retrieval chunks, and the user's onboarding profile. These inputs are
        injected into a structured prompt for <strong>Google Gemini</strong> to produce concise, empathetic guidance while
        respecting safety constraints and avoiding medical advice.
      </p>
    </section>

    <section class="content-section">
      <h2>Data Strategy & API</h2>
      <p>
        User profiles, onboarding data, and chat history are stored in MongoDB. Chat history is queried for the most recent
        exchanges to provide short-term conversational memory, while questionnaire data supplies stable personalization signals.
        Media assets are isolated in Google Cloud Storage with user-specific prefixes.
      </p>
      <p>
        The backend exposes modular endpoints for each modality:
      </p>
      <ul>
        <li><code>POST /respond</code> - text chat response with RAG and chat history</li>
        <li><code>POST /detect_video_emotions</code> - video upload with facial emotion analysis</li>
        <li><code>GET /process_speech</code> - process recent voice chunks and return a final response</li>
        <li><code>POST /api/audio</code> - Next.js route to upload audio frames to GCS</li>
      </ul>
    </section>

    <section class="content-section">
      <h2>Tech Stack</h2>
      <ul>
        <li><strong>Frontend:</strong> Next.js 15, React 19, Tailwind CSS, Clerk</li>
        <li><strong>Backend:</strong> FastAPI, Uvicorn, Python 3.12</li>
        <li><strong>AI/ML:</strong> Google Gemini, DeepFace, Wav2Vec2, Sentence Transformers, FAISS</li>
        <li><strong>Data:</strong> MongoDB Atlas, Google Cloud Storage</li>
        <li><strong>Audio:</strong> Librosa, Pydub, FFmpeg, SpeechRecognition</li>
      </ul>
      <p>
        The ML stack is intentionally modular: each modality (text, voice, video) produces interpretable intermediate
        outputs that can be logged, analyzed, and improved independently without rebuilding the entire system.
      </p>
    </section>

    <section class="content-section">
      <h2>Conclusion</h2>
      <p>
        AI-Counselling demonstrates how a modern, multimodal assistant can blend emotional signals with grounded
        knowledge to deliver supportive, personalized guidance. The project brings together real-time media
        processing, retrieval-augmented generation, and a thoughtful onboarding experience to create a companion that
        feels responsive and safe.
      </p>
    </section>
  </article>
</main>

<footer>
  <p>Designed and Developed by <span>Madhu Siddharth Suthagar</span></p>
</footer>
<script type="text/javascript" src="../js/main.js"></script>
</body>
</html>